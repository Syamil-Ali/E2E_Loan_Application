{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82f8b776",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from mlflow import MlflowClient\n",
    "import pprint\n",
    "import os\n",
    "import shutil\n",
    "import pickle\n",
    "import sys\n",
    "\n",
    "sys.path.remove('c:\\\\users\\\\user\\\\desktop\\\\python project\\\\mlflow\\\\real loan application')\n",
    "\n",
    "from utils import utils as ut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29d8e1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import basic\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# import mlflow\n",
    "import mlflow\n",
    "from mlflow import MlflowClient\n",
    "from mlflow.models import infer_signature\n",
    "\n",
    "# import hyper-parameter tuning tool\n",
    "import optuna\n",
    "\n",
    "# import compile steps\n",
    "from dsteps import data_ingestion as di\n",
    "from dsteps import data_transformation as dt\n",
    "from dsteps import model_training as mt\n",
    "import mlflow_exp as mle\n",
    "\n",
    "\n",
    "# model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "# model validation include cross validation score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "\n",
    "# os\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c5889b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the tracking uri\n",
    " \n",
    "mlflow.set_tracking_uri(\"http://127.0.0.1:5000\") # -> other than clien, high level api\n",
    "client = MlflowClient() # -> for CRUD experiments, runs, model version and registered model, low level api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "013d07ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up an experiemnt\n",
    "\n",
    "experiment_name = \"EX_3_Loan_Application_Classification\"\n",
    "\n",
    "run_name = 'base_experiment'\n",
    "\n",
    "n_trials = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a0e645b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment EX_3_Loan_Application_Classification created\n"
     ]
    }
   ],
   "source": [
    "# create experiment\n",
    "\n",
    "experiment_description = (\n",
    "    \"This is the third experiment of loan prediction project \\n\"\n",
    "    \"This experiment contains the base experiments made\"\n",
    ")\n",
    "\n",
    "\n",
    "model_involved = (\n",
    "    \"Logistic Regression\"\n",
    "    \"AdaBoost\"\n",
    ")\n",
    "\n",
    "other_tag = (\n",
    "    \"Optuna\"\n",
    ")\n",
    "\n",
    "# Provide searchable tags that define characteristics of the Runs that\n",
    "# will be in this Experiment\n",
    "experiment_tags = {\n",
    "    \"project_name\": \"loan_application_project\",\n",
    "    \"team\": \"syamil\",\n",
    "    \"project_quarter\": \"Q1-2024\",\n",
    "    \"mlflow.note.content\": experiment_description,\n",
    "    \"mode_involved\" : model_involved,\n",
    "    \"other_tag\" : other_tag\n",
    "}\n",
    "\n",
    "# Create the Experiment, providing a unique name\n",
    "try:\n",
    "    create_experiment = client.create_experiment(\n",
    "       name=experiment_name, tags=experiment_tags\n",
    "   )\n",
    "    print(f'Experiment {experiment_name} created')#\n",
    "\n",
    "except:\n",
    "    print(f'Experiment {experiment_name} already_exist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bc5a1c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning and Preprocessing Model Here\n",
    "\n",
    "\n",
    "file_path = '../data/Loan_Data.csv'\n",
    "\n",
    "df = di.data_ingest(file_path)\n",
    "X_train, y_train, X_test, y_test, encoder, scaler = dt.cleaning_train_pipeline(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f812bbf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optuna Experiment\n",
    "\n",
    "optuna_study = {}\n",
    "\n",
    "\n",
    "# Optuna code\n",
    "\n",
    "def objective(trial, name, model):\n",
    "    \n",
    "\n",
    "        # Adaboost classification\n",
    "        if name == 'AdaBoost':\n",
    "            params = {\n",
    "                'n_estimators' : trial.suggest_int('n_estimators',100, 1000),\n",
    "                'learning_rate': trial.suggest_float('learning_rate',0.0, 1.0),\n",
    "                'algorithm' : trial.suggest_categorical('algorithm',['SAMME','SAMME.R'])\n",
    "            }\n",
    "            \n",
    "        # Logistic Regression\n",
    "        elif name == 'Logistic_Regression':    \n",
    "\n",
    "            penalty_choices = ['l1', 'l2', 'elasticnet']\n",
    "\n",
    "            params = {\n",
    "                #'penalty' : trial.suggest_categorical('penalty', [None, 'l2', 'l1', 'elasticnet']),\n",
    "                'class_weight': trial.suggest_categorical('class_weight', ['balanced', None]),\n",
    "                'solver': trial.suggest_categorical('solver', ['lbfgs','liblinear','newton-cg','newton-cholesky','sag','saga']),\n",
    "                'max_iter': trial.suggest_int('max_iter', 100, 1000)\n",
    "\n",
    "            }\n",
    "\n",
    "\n",
    "        model = model.set_params(**params)\n",
    "\n",
    "        score = cross_val_score(model, X_train, y_train, cv=5, scoring='accuracy').mean()\n",
    "        \n",
    "        return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4fb7f51f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-03 11:24:12,381] A new study created in memory with name: no-name-2774a0f1-3a5c-423d-acbc-ad778ac331d0\n",
      "[I 2024-06-03 11:24:12,428] Trial 0 finished with value: 0.7543972199946538 and parameters: {'class_weight': 'balanced', 'solver': 'newton-cg', 'max_iter': 252}. Best is trial 0 with value: 0.7543972199946538.\n",
      "[I 2024-06-03 11:24:12,452] Trial 1 finished with value: 0.7543972199946538 and parameters: {'class_weight': 'balanced', 'solver': 'liblinear', 'max_iter': 253}. Best is trial 0 with value: 0.7543972199946538.\n",
      "[I 2024-06-03 11:24:12,478] Trial 2 finished with value: 0.7986367281475542 and parameters: {'class_weight': None, 'solver': 'sag', 'max_iter': 364}. Best is trial 2 with value: 0.7986367281475542.\n",
      "Registered model 'Logistic_Regression_best_params' already exists. Creating a new version of this model...\n",
      "2024/06/03 11:24:20 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: Logistic_Regression_best_params, version 5\n",
      "Created version '5' of model 'Logistic_Regression_best_params'.\n",
      "[I 2024-06-03 11:24:20,580] A new study created in memory with name: no-name-578285c8-ef8f-401e-a066-16d2f49f0d74\n",
      "[I 2024-06-03 11:24:31,034] Trial 0 finished with value: 0.7869286287089012 and parameters: {'n_estimators': 984, 'learning_rate': 0.7678539884129196, 'algorithm': 'SAMME'}. Best is trial 0 with value: 0.7869286287089012.\n",
      "[I 2024-06-03 11:24:37,647] Trial 1 finished with value: 0.7986634589681904 and parameters: {'n_estimators': 706, 'learning_rate': 0.21100370162821958, 'algorithm': 'SAMME'}. Best is trial 1 with value: 0.7986634589681904.\n",
      "[I 2024-06-03 11:24:45,801] Trial 2 finished with value: 0.7568297246725474 and parameters: {'n_estimators': 632, 'learning_rate': 0.35961443143420546, 'algorithm': 'SAMME.R'}. Best is trial 1 with value: 0.7986634589681904.\n",
      "C:\\ProgramData\\Anaconda3\\envs\\mlflowtest\\lib\\site-packages\\_distutils_hack\\__init__.py:18: UserWarning: Distutils was imported before Setuptools, but importing Setuptools also replaces the `distutils` module in `sys.modules`. This may lead to undesirable behaviors or errors. To avoid these issues, avoid using distutils directly, ensure that setuptools is installed in the traditional way (e.g. not an editable install), and/or make sure that setuptools is always imported before distutils.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\mlflowtest\\lib\\site-packages\\_distutils_hack\\__init__.py:33: UserWarning: Setuptools is replacing distutils.\n",
      "  warnings.warn(\"Setuptools is replacing distutils.\")\n",
      "Registered model 'AdaBoost_best_params' already exists. Creating a new version of this model...\n",
      "2024/06/03 11:24:53 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: AdaBoost_best_params, version 5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created version '5' of model 'AdaBoost_best_params'.\n"
     ]
    }
   ],
   "source": [
    "# trying to combine two classifier\n",
    "\n",
    "names = [\n",
    "    'Logistic_Regression',\n",
    "    'AdaBoost'\n",
    "]\n",
    "\n",
    "models = [\n",
    "    LogisticRegression(),\n",
    "    AdaBoostClassifier()\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "# set the experiment name here\n",
    "# set experiment\n",
    "mlflow.set_experiment(experiment_name)\n",
    "\n",
    "for name, model in zip(names, models):\n",
    "    \n",
    "    \n",
    "    with mlflow.start_run(run_name=f'run_name_{name}') as parent_run:\n",
    "        \n",
    "        # get mlflow id\n",
    "        run_id = parent_run.info.run_id\n",
    "        \n",
    "        #score = model_training(model,X_train, y_train, X_test, y_test)\n",
    "        study = optuna.create_study(direction='maximize', sampler=optuna.samplers.RandomSampler())\n",
    "        objective_fn = lambda trial: objective(trial, name, model)\n",
    "        study.optimize(objective_fn, n_trials=n_trials)\n",
    "        \n",
    "        \n",
    "        ##  ---- Trial ------ ##\n",
    "        \n",
    "        # log the experiment result into mlflow\n",
    "        for i, trial in enumerate(study.trials):\n",
    "            with mlflow.start_run(run_name=f\"{run_name}_{name}_trial_{i+1}\",nested=True):\n",
    "                mlflow.log_params(trial.params)\n",
    "                mlflow.log_metric('accuracy', trial.value)    \n",
    "            \n",
    "                \n",
    "        ##  ---- Champion Model ------ ##\n",
    "        \n",
    "        # update the best model run with unseen data\n",
    "        model, score_train, score_valid, signature = mt.train_model(model, X_train, y_train, X_test, y_test, study.best_params)\n",
    "\n",
    "        # update the run in experiemnt\n",
    "        exp_run_param = {\n",
    "            'name' : f'{experiment_name}',\n",
    "            'run_name': f'{run_name}_{name}_best_params',\n",
    "            'artifact_path': f'loan_application_model/{run_name}',\n",
    "            'model_name': f'{name}_best_params', \n",
    "            'signature': signature\n",
    "        }\n",
    "        \n",
    "        model_params = study.best_params\n",
    "        \n",
    "        metrics = {\n",
    "            'Accuracy Training' : score_train,\n",
    "            'Accuracy Test' : score_valid \n",
    "        }\n",
    "        \n",
    "        mle.mlflow_logging(exp_run_param, model, model_params, metrics)\n",
    "\n",
    "        optuna_study[name] = study\n",
    "\n",
    "        \n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa68b3ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af24df11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# after updating everything and testing, validating the model in mlflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5c2bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# maybe can consider getting model that are\n",
    "# 1. latest\n",
    "# 2. best\n",
    "\n",
    "\n",
    "# update the file"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlflowtest",
   "language": "python",
   "name": "mlflowtest"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
